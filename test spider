from urlparse import urljoin, urlparse
import urllib
import re
from scrapy import Request
from scrapy.item import Item, Field
from scrapy.loader import ItemLoader
from scrapy.loader.processors import Identity
from scrapy.spiders.crawl import CrawlSpider
from scrapylib.processors import default_input_processor, default_output_processor
import requests
from bs4 import BeautifulSoup

__author__ = 'ttomlins'


class NormalizedJoin(object):
    """ Strips non-empty values and joins them with the given separator. """

    def __init__(self, separator=u' ', return_list=False):
        self.separator = separator
        self.return_list = return_list

    def __call__(self, values):
        result = self.separator.join(
            [value.strip() for value in values if value and not value.isspace()])
        if self.return_list:
            return [result]
        else:
            return result


class JobItem(Item):
    # required fields
    title = Field()
    # a unique id for the job on the crawled site.
    job_id = Field()
    # the url the job was crawled from
    url = Field()
    # name of the company where the job is.
    company = Field()

    # location of the job.
    # should ideally include city, state and country.
    # postal code if available.
    # does not need to include street information
    location = Field()
    description = Field()

    # the url users should be sent to for viewing the job. Sometimes
    # the "url" field requires a cookie to be set and this "apply_url" field will be differnt
    # since it requires no cookie or session state.
    apply_url = Field()

    # optional fields
    industry = Field()
    baseSalary = Field()
    benefits = Field()
    requirements = Field()
    skills = Field()
    work_hours = Field()


class JobItemLoader(ItemLoader):
    default_item_class = JobItem
    default_input_processor = default_input_processor
    default_output_processor = default_output_processor
    # all text fields are joined.
    description_in = Identity()
    description_out = NormalizedJoin()
    requirements_in = Identity()
    requirements_out = NormalizedJoin()
    skills_in = Identity()
    skills_out = NormalizedJoin()
    benefits_in = Identity()
    benefits_out = NormalizedJoin()


REF_REGEX = re.compile(r'\/(\d+)$')

APPEND_GB = lambda x: x.strip() + ", GB"


class SimplyLawJobs(CrawlSpider):
    """ Should navigate to the start_url, paginate through
    the search results pages and visit each job listed.
    For every job details page found, should produce a JobItem
    with the relevant fields populated.

    You can use the Rule system for CrawSpider (the base class)
    or you can manually paginate in the "parse" method that is called
    after the first page of search results is loaded from the start_url.

    There are some utilities above like "NormalizedJoin" and JobItemLoader
    to help making generating clean item data easier.
    """
    
    start_urls = ["http://www.simplylawjobs.com/jobs"]
    name = 'lawjobsspider'
    
    
    #----------------#
    #----------------#
    #-YOUR CODE HERE-#
    #----------------#
    #----------------#

    #MyFinalLogic

    rules = (Rule (LinkExtractor(allow=(),restrict_xpaths=('//*[@id="search_results_list"]/ul/li[1]',))
    , callback="parse_items", follow= True,),)

    def parse_items(self, response):
    hxs = HtmlXPathSelector(response)
    titles = hxs.select("//html")
    items = []
    for titles in titles:
      item = JobItem()
      item ["title"] = titles.select("//*[@id="search_results_list"]/ul/li[1]/div[2]/div[1]/a[1]/h2").extract()
      item ["company"] = titles.select("//*[@id="search_results_list"]/ul/li[1]/div[2]/div[1]/a[2]").extract()
      item ["location"] = titles.select("//*[@id="search_results_list"]/ul/li[1]/div[2]/div[1]/p/span").extract()
      item ["salary"] = titles.select("//*[@id="search_results_list"]/ul/li[1]/div[2]/div[1]/p/text()").extract()
      item ["desciption"] = titles.select("//*[@id="search_results_list"]/ul/li[1]/p[1]/text()") 

      items.append(item)
      return(items)


    
    #LogicTrial 3
    """rules = (
        Rule(LinkExtractor(restrict_xpaths = "(//div[@class='job']/ul/li)"), callback="parse_item"),
    )

    my_request = requests.get(start_urls)
    soup = BeautifulSoup(my_request.content)

    search_data = soup.find_all("li", {"class" : "job"})

    for item in search_data:
        Time = item.find_all("div", {"class" : "time"})
        job_title = item.find_all("a/@href/h2")
        company_title = item.find_all("a", {"class" : "title"})
        Location = item.find_all("p",{"span" : "text()"})"""



    
    #Logic Trial 2
    """rules = (
    Rule(LinkExtractor(allow =('//div[@class = "job"]'), restrict_xpaths = ()), callback = "parse_item"),
    )

    def parse_item(self, response):
        for each in response.xpath('//ul/li'):
            title = each.xpath('//div[@class = "title"]').extract()
            companyname = sel.xpath('a/@href').extract()
            salary = sel.xpath('/html/body/p/br/text()').extract()
            print title, companyname, salary"""
            
    #LogicTrial 1
    """r = requests.get("http://www.simplylawjobs.com/jobs")
    soup = BeautifulSoup(r.content,"lxml")
    #print soup.prettify()    
    search_data = soup.find_all("li", {"class":"job"})
    search_data1 = soup.find_all("li", {"class":"job"})
    #for eachdata in search_data:
    #each_data = soup.findall("a", {"class":"title"})
    #print each_data"""
        

